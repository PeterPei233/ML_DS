{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Junsheng Pei\n",
    "    Zihao Yang\n",
    "    Nuochen Lyu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> DS200A Computer Vision Assignment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>  Part Four: Extension Activities</h2>\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow (Optional)- Now, try using TensorFlow to categorize your images. The accuracy should be significantly higher due to the usage of nueral nets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "# Derived from models in `https://github.com/kuangliu/pytorch-cifar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import linear_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --\n",
    "# CLI\n",
    "# The default values of most arguments are sufficient for this problem\n",
    "# You may want to use --subset-train or --subset-val to use only a subset\n",
    "# of the data while testing your code.\n",
    "# specific location\n",
    "\n",
    "#### This model ran quite slow in my jupyter-notebook, so I set the epochs to 1. If you want to test it, please\n",
    "####copy all the code into .py and ran it in terminal.\n",
    "epochs =23\n",
    "subset_val = None\n",
    "batch_size = 128\n",
    "subset_train = None\n",
    "gpu = False\n",
    "seed = 123\n",
    "classes = ('airplanes','bear','blimp','comet','crab','dog','dolphin''giraffe',\n",
    "'goat','gorilla','kangaroo','killer-whale','leopards','llama','penguin','porcupine','teddy-bear',\n",
    "'triceratops','unicorn','zebra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cifar10.py: making dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "def load_data():\n",
    "    print('cifar10.py: making dataloaders...', file=sys.stderr)\n",
    "    # transforms define preprocessing on the dataset\n",
    "    transform_train = transforms.Compose([\n",
    "#         # random crop\n",
    "        transforms.RandomCrop(64),\n",
    "#         #  random horizontal flip\n",
    "         transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std= (0.24705882352941178, 0.24352941176470588, 0.2615686274509804))\n",
    "    ])\n",
    "    transform_val = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std= (0.24705882352941178, 0.24352941176470588, 0.2615686274509804))\n",
    "    ])\n",
    "    # loading the dataset\n",
    "    try:\n",
    "#         trainset = datasets.CIFAR10(root=args.data_dir, train=True, download=True, transform=transform_train)\n",
    "#         valset  = datasets.CIFAR10(root=args.data_dir, train=False, download=True, transform=transform_val)\n",
    "        dataset = datasets.ImageFolder(root='./20_categories_training',transform=transform_train)\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        trainset, valset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    except:\n",
    "        raise Exception('error loading data -- try rerunning w/ `--download` flag')\n",
    "    # selecting a random subset of the dataset if requested\n",
    "    np.random.seed(seed)\n",
    "    if (subset_train is not None):\n",
    "        idxs = np.random.choice(len(trainset), subset_train, replace=False)\n",
    "        trainset = torch.utils.data.Subset(trainset, idxs)\n",
    "    if (subset_val is not None):\n",
    "        idxs = np.random.choice(len(valset), subset_val, replace=False)\n",
    "        trainset = torch.utils.data.Subset(valset, idxs)\n",
    "    # data loading objects\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        valset,\n",
    "        batch_size=512,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    dataloaders = {\n",
    "        \"train\" : trainloader,\n",
    "        \"val\"  : valloader,\n",
    "    }\n",
    "    return dataloaders\n",
    "\n",
    "dataloaders = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --\n",
    "# Model definition\n",
    "# Derived from models in `https://github.com/kuangliu/pytorch-cifar`\n",
    "class PreActBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bn1   = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        return out + shortcut\n",
    "\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_blocks=[2, 2, 2, 2], num_classes=20):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.prep = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            self._make_layer(64, 64, num_blocks[0], stride=1),\n",
    "            self._make_layer(64, 128, num_blocks[1], stride=2),\n",
    "            self._make_layer(128, 256, num_blocks[2], stride=2),\n",
    "            self._make_layer(256, 256, num_blocks[3], stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "\n",
    "        strides = [stride] + [1] * (num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(PreActBlock(in_channels=in_channels, out_channels=out_channels, stride=stride))\n",
    "            in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.prep(x)\n",
    "        x = self.layers(x)\n",
    "\n",
    "        x_avg = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x_avg = x_avg.view(x_avg.size(0), -1)\n",
    "\n",
    "        x_max = F.adaptive_max_pool2d(x, (1, 1))\n",
    "        x_max = x_max.view(x_max.size(0), -1)\n",
    "\n",
    "        x = torch.cat([x_avg, x_max], dim=-1)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# --\n",
    "# Learning rate functions\n",
    "def set_lr(lr, optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "#lr = calc_lr(lr, epoch, i, N)\n",
    "def const_lr_maker(const=0.001):\n",
    "    # TO IMPLEMENT Part (e)\n",
    "    def f(curr_lr, epoch, batch_idx, N):\n",
    "        return const\n",
    "    return f\n",
    "\n",
    "def anneal_lr_maker(lr=0.1, factor=0.99):\n",
    "    # TO IMPLEMENT Part (e)\n",
    "    def f(curr_lr, epoch, batch_idx, N):\n",
    "        return curr_lr * factor\n",
    "    return f\n",
    "\n",
    "def special_lr_maker(hp_max=0.1, epochs=3, hp_init=0.0, hp_final=0.005, extra=5):\n",
    "    def f(curr_lr, epoch, batch_idx, N):\n",
    "        progress = epoch + batch_idx / N\n",
    "        if progress < epochs / 2:\n",
    "            return 2 * hp_max * (1 - (epochs - progress) / epochs)\n",
    "        elif progress <= epochs:\n",
    "            return hp_final + 2 * (hp_max - hp_final) * (epochs - progress) / epochs\n",
    "        elif progress <= epochs + extra:\n",
    "            return hp_final * (extra - (progress - epochs)) / extra\n",
    "        else:\n",
    "            return hp_final / 10\n",
    "    return f\n",
    "\n",
    "const_lr = const_lr_maker()\n",
    "anneal_lr = anneal_lr_maker()\n",
    "special_lr = special_lr_maker(epochs=epochs+3)\n",
    "\n",
    "# --\n",
    "# Model training\n",
    "def train_with_lr_scheme(calc_lr):\n",
    "\n",
    "    print('cifar10.py: initializing model...', file=sys.stderr)\n",
    "    if (torch.cuda.is_available() and gpu):\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    model = ResNet18().to(device)\n",
    "    model.verbose = True\n",
    "    # --\n",
    "    # Initialize optimizer\n",
    "    print('cifar10.py: training...', file=sys.stderr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    lr = 0.01\n",
    "\n",
    "    trainloader, valloader = dataloaders['train'], dataloaders['val']\n",
    "    N = len(trainloader)\n",
    "    iter_count = 0 # a count of all iterations\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        total_train = 0; correct_train = 0\n",
    "        total_val = 0; correct_val = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if (torch.cuda.is_available() and gpu):\n",
    "                labels = labels.cuda()\n",
    "                inputs = inputs.cuda()\n",
    "            # forward + backward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # optimize\n",
    "            lr = calc_lr(lr, epoch, i, N)\n",
    "            set_lr(lr, optimizer)\n",
    "            iter_count += 1\n",
    "            optimizer.step()\n",
    "\n",
    "            # compute training statistics\n",
    "            logits = outputs.cpu().detach().numpy()\n",
    "            y_pred_train = np.argmax(logits, axis=1)\n",
    "            y_train = labels.cpu().detach().numpy()\n",
    "            total_train += y_train.shape[0]\n",
    "            correct_train += sum(y_pred_train == y_train)\n",
    "            running_loss += loss.item()\n",
    "            print(\"Epoch:\", epoch, \"\\tMiniBatch:\", i, \"\\tPartial Training Accuracy:\", correct_train/total_train,  \"\\tRunning Loss:\", running_loss/(i+1))\n",
    "        print(\"Epoch:\", epoch, \"\\tFinal Training Accuracy:\", {correct_train/total_train})\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            if (torch.cuda.is_available() and gpu):\n",
    "                labels = labels.cuda()\n",
    "                inputs = inputs.cuda()\n",
    "            # predict outputs\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.cpu().detach().numpy()\n",
    "\n",
    "            # compute validation statistics\n",
    "            y_pred_train = np.argmax(logits, axis=1)\n",
    "            y_train = labels.cpu().detach().numpy()\n",
    "            total_val += y_train.shape[0]\n",
    "            correct_val  += sum(y_pred_train == y_train)\n",
    "            print(\"Epoch:\", epoch, \"\\tMiniBatch:\", i, \"\\tPartial Validation Accuracy:\", correct_val/total_val)\n",
    "        print(\"Epoch:\", epoch, \"\\tFinal Validation Accuracy:\", {correct_val/total_val})\n",
    "        result = {}\n",
    "        result['train_accuracy'] = correct_train/total_train\n",
    "        result['val_accuracy'] = correct_val/total_val\n",
    "        result['num_epochs'] = epochs\n",
    "        result['train_loss'] = running_loss\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== Training network with an annealing LR scheme ====\")\n",
    "result_anneal = train_with_lr_scheme(anneal_lr)\n",
    "result_anneal['lr_scheme'] = 'anneal'\n",
    "print(\"===== Training network with a special LR scheme ====\")\n",
    "result_special = train_with_lr_scheme(special_lr)\n",
    "result_special['lr_scheme'] = 'special_0'\n",
    "print(\"==== Summary ===\")\n",
    "df = pd.DataFrame([result_const, result_anneal, result_special])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I run the code in terminal and have the final reults below:\n",
    "\n",
    "#Epoch: 19   MiniBatch: 8    Partial Training Accuracy: 0.3585069444444444   Running Loss: 2.0570848253038196\n",
    "#Epoch: 19   MiniBatch: 9    Partial Training Accuracy: 0.3591666666666667   Running Loss: 2.0550736665725706\n",
    "#Epoch: 19   Final Training Accuracy: {0.3591666666666667}\n",
    "#Epoch: 19   MiniBatch: 0    Partial Validation Accuracy: 0.37209302325581395\n",
    "#Epoch: 19   Final Validation Accuracy: {0.37209302325581395}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The validation accuracy is 37.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
